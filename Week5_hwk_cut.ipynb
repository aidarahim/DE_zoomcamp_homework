{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07de9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5bbb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/aidarahim/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/28 00:55:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6894312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3cab876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7653f",
   "metadata": {},
   "source": [
    "Question 1: PySpark Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d38cab59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c382781f",
   "metadata": {},
   "source": [
    "Question 2: Size of fhvhv folder Feb 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c7ef24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210M\tfhvhv/2021/02\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh fhvhv/2021/02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8a17d",
   "metadata": {},
   "source": [
    "Question 3: Record count on Feb 15, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ba634ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('fhvhv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "738a1e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|ct_feb15|\n",
      "+--------+\n",
      "|  367170|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*) as ct_feb15\n",
    "FROM\n",
    "    fhvhv_data\n",
    "WHERE DATE(pickup_datetime) between '2021-02-15' and '2021-02-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1089beb6",
   "metadata": {},
   "source": [
    "Question 4: Day with the longest trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9166b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 9:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|    pickup_datetime|duration|\n",
      "+-------------------+--------+\n",
      "|2021-02-11 13:40:44|  1259.0|\n",
      "+-------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_datetime, ((bigint(to_timestamp(dropoff_datetime)))-(bigint(to_timestamp(pickup_datetime))))/(60) as duration\n",
    "FROM\n",
    "    fhvhv_data\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90be04",
   "metadata": {},
   "source": [
    "Question 5: stages (for most frequent `dispatching_base_num`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f37c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "|dispatching_base_num|ct_dispatching_base_num|\n",
      "+--------------------+-----------------------+\n",
      "|              B02876|                 215693|\n",
      "|              B03136|                   1741|\n",
      "|              B02877|                 198938|\n",
      "|              B02869|                 429720|\n",
      "|              B02883|                 251617|\n",
      "|              B02835|                 189031|\n",
      "|              B02884|                 244963|\n",
      "|              B02880|                 115716|\n",
      "|              B02878|                 305185|\n",
      "|              B02836|                 128978|\n",
      "|              B02872|                 882689|\n",
      "|              B02512|                  41043|\n",
      "|              B02867|                 200530|\n",
      "|              B02866|                 311089|\n",
      "|              B02871|                 312364|\n",
      "|              B02889|                 138762|\n",
      "|              B02844|                   3502|\n",
      "|              B02510|                3233664|\n",
      "|              B02888|                 169167|\n",
      "|              B02682|                 303255|\n",
      "+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    dispatching_base_num, \n",
    "    COUNT(*) as ct_dispatching_base_num\n",
    "FROM \n",
    "    fhvhv_data\n",
    "GROUP BY 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af42448",
   "metadata": {},
   "source": [
    "Question 6: Most common locations pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b67b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec8e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('df')\n",
    "df_zones.registerTempTable('df_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6a73ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+------------+\n",
      "|LocationID| Borough|                Zone|service_zone|\n",
      "+----------+--------+--------------------+------------+\n",
      "|        76|Brooklyn|       East New York|   Boro Zone|\n",
      "|        77|Brooklyn|East New York/Pen...|   Boro Zone|\n",
      "+----------+--------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM df_zones\n",
    "WHERE Zone LIKE \"%East New York%\"\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aef9b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:========================================>             (150 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|        concat_zones|ct_concat_zones|\n",
      "+--------------------+---------------+\n",
      "|East New York / E...|          45041|\n",
      "|Borough Park / Bo...|          37329|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 23:====================================================> (193 + 4) / 200]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT concat_zones, COUNT(*) as ct_concat_zones\n",
    "FROM\n",
    "(SELECT dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, PUzone, DOLocationID, Zone as DOzone,\n",
    "    CONCAT(PUzone, ' / ', Zone) as concat_zones\n",
    "FROM\n",
    "(SELECT dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, Zone as PUzone, DOLocationID\n",
    "FROM\n",
    "    df t\n",
    "LEFT JOIN df_zones zpu ON t.PULocationID = zpu.LocationID) t2\n",
    "LEFT JOIN df_zones zdo ON t2.DOLocationID = zdo.LocationID) t3\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c7f5e",
   "metadata": {},
   "source": [
    "EXTRA - try join df's first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48d029c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join1 = df.join(df_zones, df.PULocationID == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd1601a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join2 = df_join1.join(df_zones, df_join1.DOLocationID == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac40373",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join2.registerTempTable('df_join2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
